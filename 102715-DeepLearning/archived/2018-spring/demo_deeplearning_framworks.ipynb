{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N, D, H = 100, 10, 20\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "W = np.random.randn(D, H)\n",
    "b = np.random.randn(N, H)\n",
    "\n",
    "a = x.dot(W)\n",
    "c = a + b\n",
    "l = np.sum(c)\n",
    "\n",
    "grad_l = 1.\n",
    "grad_c = grad_l * np.ones((N, H))\n",
    "grad_a = grad_c.copy()\n",
    "grad_b = grad_c.copy()\n",
    "grad_x = grad_a.dot(W.T)\n",
    "grad_W = x.T.dot(grad_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/kaitopia/3-software/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device = \"/gpu:0\" if use_cuda else \"/cpu:0\"\n",
    "with tf.device(device):\n",
    "    tf_tensor_x = tf.convert_to_tensor(x)\n",
    "    tf_tensor_W = tf.convert_to_tensor(W)\n",
    "    tf_tensor_b = tf.convert_to_tensor(b)\n",
    "\n",
    "    tf_tensor_a = tf.matmul(tf_tensor_x, tf_tensor_W)\n",
    "    tf_tensor_c = tf_tensor_a + tf_tensor_b\n",
    "    tf_tensor_l = tf.reduce_sum(tf_tensor_c)\n",
    "\n",
    "tf_tensor_grad_x, tf_tensor_grad_W, tf_tensor_grad_b = \\\n",
    "    tf.gradients(tf_tensor_l, [tf_tensor_x, tf_tensor_W, tf_tensor_b])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([tf_tensor_grad_x, tf_tensor_grad_W, tf_tensor_grad_b])\n",
    "    tf_val_grad_x, tf_val_grad_W, tf_val_grad_b = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "if use_cuda:\n",
    "    torch_variable_x = Variable(torch.from_numpy(x).cuda(), requires_grad=True)\n",
    "    torch_variable_W = Variable(torch.from_numpy(W).cuda(), requires_grad=True)\n",
    "    torch_variable_b = Variable(torch.from_numpy(b).cuda(), requires_grad=True)\n",
    "else:\n",
    "    torch_variable_x = Variable(torch.from_numpy(x), requires_grad=True)\n",
    "    torch_variable_W = Variable(torch.from_numpy(W), requires_grad=True)\n",
    "    torch_variable_b = Variable(torch.from_numpy(b), requires_grad=True)\n",
    "\n",
    "torch_variable_a = torch_variable_x.matmul(torch_variable_W)\n",
    "torch_variable_c = torch_variable_a + torch_variable_b\n",
    "torch_variable_l = torch.sum(torch_variable_c)\n",
    "\n",
    "torch_variable_l.backward()\n",
    "\n",
    "torch_val_grad_x = torch_variable_x.grad.data\n",
    "torch_val_grad_W = torch_variable_W.grad.data\n",
    "torch_val_grad_b = torch_variable_b.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff of grad x(numpy vs tf): 0.0\n",
      "diff of grad W(numpy vs tf): 0.0\n",
      "diff of grad b(numpy vs tf): 0.0\n",
      "diff of grad x(numpy vs torch): 0.0\n",
      "diff of grad x(numpy vs torch): 0.0\n",
      "diff of grad x(numpy vs torch): 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"diff of grad x(numpy vs tf): {}\".format(np.linalg.norm(grad_x - tf_val_grad_x)))\n",
    "print(\"diff of grad W(numpy vs tf): {}\".format(np.linalg.norm(grad_W - tf_val_grad_W)))\n",
    "print(\"diff of grad b(numpy vs tf): {}\".format(np.linalg.norm(grad_b - tf_val_grad_b)))\n",
    "print(\"diff of grad x(numpy vs torch): {}\".format(np.linalg.norm(grad_x - torch_val_grad_x)))\n",
    "print(\"diff of grad x(numpy vs torch): {}\".format(np.linalg.norm(grad_W - torch_val_grad_W)))\n",
    "print(\"diff of grad x(numpy vs torch): {}\".format(np.linalg.norm(grad_b - torch_val_grad_b)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
